{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQMGJZ61s18c"
      },
      "source": [
        "# Multi-class classification\n",
        "\n",
        "look at description of movie, and predict the genre that it might belong to  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iqGie29CstgI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17w4pkK8syfD"
      },
      "source": [
        "# Kaggle data link\n",
        "get data from the Genre Classification dataset, combine train and test, and save it in movie_data.csv (commented out the code after saving the csv file)\n",
        "\n",
        "link to the original data from kaggle - \n",
        "https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id                           title       genre  \\\n",
            "0   1   Oscar et la dame rose (2009)       drama    \n",
            "1   2                   Cupid (1997)    thriller    \n",
            "\n",
            "                                         description  \n",
            "0   Listening in to a conversation between his do...  \n",
            "1   A brother and sister with a past incestuous r...  \n",
            "   id                       title       genre  \\\n",
            "0   1       Edgar's Lunch (1998)    thriller    \n",
            "1   2   La guerra de pap√° (1977)      comedy    \n",
            "\n",
            "                                         description  \n",
            "0   L.R. Brane loves his life - his car, his apar...  \n",
            "1   Spain, March 1964: Quico is a very naughty ch...  \n"
          ]
        }
      ],
      "source": [
        "train=pd.read_csv('Genre Classification Dataset/train_data.txt',sep=':::',header=None,names=['id','title','genre','description'], engine='python')\n",
        "test=pd.read_csv('Genre Classification Dataset/test_data_solution.txt',sep=':::',header=None,names=['id','title','genre','description'], engine='python')\n",
        "print(train.head(2))\n",
        "print(test.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            title       genre  \\\n",
            "0   Oscar et la dame rose (2009)       drama    \n",
            "1                   Cupid (1997)    thriller    \n",
            "\n",
            "                                         description  \n",
            "0   Listening in to a conversation between his do...  \n",
            "1   A brother and sister with a past incestuous r...  \n"
          ]
        }
      ],
      "source": [
        "# 1. Select the desired columns:\n",
        "train_selected = train[['title', 'genre', 'description']]\n",
        "test_selected = test[['title', 'genre', 'description']]\n",
        "\n",
        "# 2. Vertically concatenate (join) the DataFrames:\n",
        "combined_df = pd.concat([train_selected, test_selected], ignore_index=True)\n",
        "combined_df.to_csv('movie_data.csv', index=False)\n",
        "print(combined_df.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhE_YFFGs5yO"
      },
      "source": [
        "# Data cleaning and exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ABmzJ62_s7zL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                title       genre  \\\n",
            "0       Oscar et la dame rose (2009)       drama    \n",
            "1                       Cupid (1997)    thriller    \n",
            "2   Young, Wild and Wonderful (1980)       adult    \n",
            "3              The Secret Sin (1915)       drama    \n",
            "4             The Unrecovered (2007)       drama    \n",
            "\n",
            "                                         description  \n",
            "0   Listening in to a conversation between his do...  \n",
            "1   A brother and sister with a past incestuous r...  \n",
            "2   As the bus empties the students for their fie...  \n",
            "3   To help their unemployed father make ends mee...  \n",
            "4   The film's title refers not only to the un-re...  \n",
            "(108414, 3)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('movie_data.csv')\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0S8jQ59s9yl"
      },
      "source": [
        "### Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLe4zAAls-Wz"
      },
      "outputs": [],
      "source": [
        "print(pd.DataFrame({'Missing Count': df.isnull().sum(), 'Missing Percentage': (df.isnull().sum() / len(df)) * 100})) #Prints a dataframe with missing counts and percentages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJz53FlUtAWb"
      },
      "source": [
        "### Checking for class imbalance  \n",
        "\n",
        "- Dataset is imbalanced, with majority classes being genres - 'drama', 'documentary', 'comedy', 'short', etc.  \n",
        "- We will choose to leave the dataset as is without any undersampling or oversampling, to reflect real-world scenarios.  \n",
        "- As there is class imbalance, accuracy would be a misleading performance metric, and thus we will use F1 score as the evaluation metric.\n",
        "- some models will naturally perform better than others, because of class imbalance.\n",
        "- can add 'future work' as trying out different ways to mitigate this problem and the results from each way.\n",
        "- show class specific performance and comment on that in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oEjJhI2tDNa"
      },
      "outputs": [],
      "source": [
        "genre_count = df['genre'].value_counts()\n",
        "\n",
        "print(genre_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "genre_count = df['genre'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(genre_count.index, genre_count.values)\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Genre Distribution')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cUSxHJgtFaP"
      },
      "source": [
        "### Distribution of description lengths.\n",
        "- appears to be normally distributed with slight skewness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWCFmXo7tIzb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df['description_length'] = df['description'].apply(len)\n",
        "\n",
        "# Create a distribution plot (histogram)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['description_length'], bins=50, kde=True)  # bins and kde can be adjusted\n",
        "plt.title('Distribution of Description Lengths')\n",
        "plt.xlabel('Description Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzxP4ui8tNyi"
      },
      "source": [
        "# Data preprocessing\n",
        "- lowercasing\n",
        "- removing punctuation\n",
        "- removing stopwords\n",
        "- lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3hNcfaYLtbct"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original vs. Processed Descriptions:\n",
            "Original:  Listening in to a conversation between his doctor and parents, 10-year-old Oscar learns what nobody has the courage to tell him. He only has a few weeks to live. Furious, he refuses to speak to anyone except straight-talking Rose, the lady in pink he meets on the hospital stairs. As Christmas approaches, Rose uses her fantastical experiences as a professional wrestler, her imagination, wit and charm to allow Oscar to live life and love to the full, in the company of his friends Pop Corn, Einstein, Bacon and childhood sweetheart Peggy Blue.\n",
            "Processed: listen convers doctor parent 10yearold oscar learn nobodi courag tell week live furious refus speak anyon except straighttalk rose ladi pink meet hospit stair christma approach rose use fantast experi profession wrestler imagin wit charm allow oscar live life love full compani friend pop corn einstein bacon childhood sweetheart peggi blue\n",
            "----------------------------------------\n",
            "Original:  A brother and sister with a past incestuous relationship have a current murderous relationship. He murders the women who reject him and she murders the women who get too close to him.\n",
            "Processed: brother sister past incestu relationship current murder relationship murder women reject murder women get close\n",
            "----------------------------------------\n",
            "Original:  As the bus empties the students for their field trip to the Museum of Natural History, little does the tour guide suspect that the students are there for more than just another tour. First, during the lecture films, the coeds drift into dreams of the most erotic fantasies one can imagine. After the films, they release the emotion of the fantasies in the most erotic and uncommon ways. One slips off to the curator's office for a little \"acquisition. \" Another finds the anthropologist to see what bones can be identified. Even the head teacher isn't immune. Soon the tour is over, but as the bus departs, everyone admits it was quite an education.\n",
            "Processed: bus empti student field trip museum natur histori littl tour guid suspect student anoth tour first lectur film co drift dream erot fantasi one imagin film releas emot fantasi erot uncommon way one slip curat offic littl acquisit anoth find anthropologist see bone identifi even head teacher isnt immun soon tour bus depart everyon admit quit educ\n",
            "----------------------------------------\n",
            "Original:  To help their unemployed father make ends meet, Edith and her twin sister Grace work as seamstresses . An invalid, Grace falls prey to the temptations of Chinatown opium and becomes an addict, a condition worsened by a misguided physician who prescribes morphine to ease her pain. When their father strikes oil, the family enjoys a new prosperity and the sisters meet the eligible Jack Herron, a fellow oil prospector. To Grace's shock, Jack falls in love with Edith and in her jealousy, Grace tells Jack that Edith, not she, has a drug problem. Hinting that her sister will soon need more morphine, Grace arranges for a dinner in Chinatown with the couple. While her sister and Jack dance, Grace slips away to an opium den. Edith follows her, but ends up in the wrong den and is arrested in an ensuing drug raid. After he bails her out of jail, Edith takes an angry Jack to search for Grace and stumbles across her half-conscious body lying in the street. The truth about the sisters is revealed, and after sending Grace to a sanitarium in the country, Jack and Edith are married.\n",
            "Processed: help unemploy father make end meet edith twin sister grace work seamstress invalid grace fall prey temptat chinatown opium becom addict condit worsen misguid physician prescrib morphin eas pain father strike oil famili enjoy new prosper sister meet elig jack herron fellow oil prospector grace shock jack fall love edith jealousi grace tell jack edith drug problem hint sister soon need morphin grace arrang dinner chinatown coupl sister jack danc grace slip away opium den edith follow end wrong den arrest ensu drug raid bail jail edith take angri jack search grace stumbl across halfconsci bodi lie street truth sister reveal send grace sanitarium countri jack edith marri\n",
            "----------------------------------------\n",
            "Original:  The film's title refers not only to the un-recovered bodies at ground zero, but also to the state of the nation at large. Set in the hallucinatory period of time between September 11 and Halloween of 2001, The Unrecovered examines the effect of terror on the average mind, the way a state of heightened anxiety and/or alertness can cause the average person to make the sort of imaginative connections that are normally made only by artists and conspiracy theorists-both of whom figure prominently in this film. The Unrecovered explores the way in which irony, empathy, and paranoia relate to one another in the wake of 9/11.\n",
            "Processed: film titl refer unrecov bodi ground zero also state nation larg set hallucinatori period time septemb 11 halloween 2001 unrecov examin effect terror averag mind way state heighten anxieti andor alert caus averag person make sort imagin connect normal made artist conspiraci theoristsboth figur promin film unrecov explor way ironi empathi paranoia relat one anoth wake 911\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK resources (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stemmer and stop words\n",
        "stemmer = SnowballStemmer('english')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a text string.\"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "    # Remove stop words and stem\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    # Join words back into a string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the 'description' column\n",
        "df['processed_description'] = df['description'].apply(preprocess_text)\n",
        "\n",
        "# Example: Print the original and processed descriptions for the first few rows\n",
        "print(\"Original vs. Processed Descriptions:\")\n",
        "for i in range(5):  # Print the first 5 rows as an example.\n",
        "    print(f\"Original: {df['description'][i]}\")\n",
        "    print(f\"Processed: {df['processed_description'][i]}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Now, use the 'processed_description' column for feature extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-MOvtCOtcTM"
      },
      "source": [
        "# Feature Extraction 1: TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3U-PKlb4teGG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Feature Shape: (108414, 5000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features\n",
        "\n",
        "# Fit and transform the processed descriptions\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['processed_description'])\n",
        "\n",
        "# Convert to a DataFrame (optional, but can be helpful for inspection)\n",
        "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the shape of the TF-IDF features\n",
        "print(\"TF-IDF Feature Shape:\", tfidf_features.shape)\n",
        "\n",
        "# Example: Print the first few rows of the TF-IDF DataFrame (if you converted it)\n",
        "# print(tfidf_df.head())\n",
        "\n",
        "# Now, tfidf_features (or tfidf_df) contains the TF-IDF representation of your text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqhcpaaatkBY"
      },
      "source": [
        "## Train-test split\n",
        "\n",
        "- 90:10 split\n",
        "- shuffle before splitting\n",
        "- stratified split to maintain class distribution in train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ayXUkjuNtkyP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (97572, 5000)\n",
            "X_test shape: (10842, 5000)\n",
            "y_train shape: (97572,)\n",
            "y_test shape: (10842,)\n",
            "\n",
            "Training genre distribution:\n",
            "genre\n",
            "drama           0.251117\n",
            "documentary     0.241596\n",
            "comedy          0.137375\n",
            "short           0.093572\n",
            "horror          0.040657\n",
            "thriller        0.029342\n",
            "action          0.024249\n",
            "western         0.019042\n",
            "reality-tv      0.016296\n",
            "family          0.014451\n",
            "adventure       0.014297\n",
            "music           0.013487\n",
            "romance         0.012401\n",
            "sci-fi          0.011930\n",
            "adult           0.010884\n",
            "crime           0.009316\n",
            "animation       0.009183\n",
            "sport           0.007963\n",
            "talk-show       0.007215\n",
            "fantasy         0.005944\n",
            "mystery         0.005873\n",
            "musical         0.005104\n",
            "biography       0.004878\n",
            "history         0.004479\n",
            "game-show       0.003567\n",
            "news            0.003341\n",
            "war             0.002439\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Testing genre distribution:\n",
            "genre\n",
            "drama           0.251153\n",
            "documentary     0.241561\n",
            "comedy          0.137336\n",
            "short           0.093617\n",
            "horror          0.040675\n",
            "thriller        0.029330\n",
            "action          0.024258\n",
            "western         0.019000\n",
            "reality-tv      0.016325\n",
            "family          0.014481\n",
            "adventure       0.014296\n",
            "music           0.013466\n",
            "romance         0.012359\n",
            "sci-fi          0.011898\n",
            "adult           0.010884\n",
            "crime           0.009316\n",
            "animation       0.009223\n",
            "sport           0.007932\n",
            "talk-show       0.007194\n",
            "fantasy         0.005995\n",
            "mystery         0.005903\n",
            "musical         0.005073\n",
            "biography       0.004888\n",
            "history         0.004519\n",
            "game-show       0.003597\n",
            "news            0.003320\n",
            "war             0.002398\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'tfidf_features' is your TF-IDF feature matrix and 'df['genre']' is your target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    tfidf_features,\n",
        "    df['genre'],\n",
        "    test_size=0.1,  # 10% for testing\n",
        "    random_state=42,\n",
        "    shuffle=True,  # Shuffle the data before splitting\n",
        "    stratify=df['genre'] # Stratified split based on the 'genre' column\n",
        ")\n",
        "\n",
        "# Print the shapes of the train and test sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "#Verify the distribution of genres in the training and testing sets.\n",
        "print('\\nTraining genre distribution:')\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print('\\nTesting genre distribution:')\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpy5W9RtqP1"
      },
      "source": [
        "## Model 1 - Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QS9YliZ1trHD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Results:\n",
            "Accuracy: 0.538553772366722\n",
            "Precision (weighted): 0.5133043959958458\n",
            "Recall (weighted): 0.538553772366722\n",
            "F1-score (weighted): 0.469730252107493\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.67      0.19      0.30       263\n",
            "       adult        0.55      0.10      0.17       118\n",
            "   adventure        0.75      0.14      0.23       155\n",
            "   animation        0.00      0.00      0.00       100\n",
            "   biography        0.00      0.00      0.00        53\n",
            "      comedy        0.54      0.47      0.50      1489\n",
            "       crime        0.00      0.00      0.00       101\n",
            " documentary        0.58      0.88      0.70      2619\n",
            "       drama        0.47      0.81      0.60      2723\n",
            "      family        0.33      0.01      0.01       157\n",
            "     fantasy        0.00      0.00      0.00        65\n",
            "   game-show        1.00      0.33      0.50        39\n",
            "     history        0.00      0.00      0.00        49\n",
            "      horror        0.73      0.44      0.55       441\n",
            "       music        0.65      0.23      0.34       146\n",
            "     musical        0.00      0.00      0.00        55\n",
            "     mystery        0.00      0.00      0.00        64\n",
            "        news        0.00      0.00      0.00        36\n",
            "  reality-tv        0.36      0.03      0.05       177\n",
            "     romance        0.00      0.00      0.00       134\n",
            "      sci-fi        0.62      0.10      0.17       129\n",
            "       short        0.52      0.13      0.20      1015\n",
            "       sport        0.82      0.16      0.27        86\n",
            "   talk-show        0.00      0.00      0.00        78\n",
            "    thriller        0.47      0.03      0.05       318\n",
            "         war        0.00      0.00      0.00        26\n",
            "     western        0.94      0.70      0.81       206\n",
            "\n",
            "     accuracy                           0.54     10842\n",
            "    macro avg       0.37      0.18      0.20     10842\n",
            " weighted avg       0.51      0.54      0.47     10842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Initialize and train the Naive Bayes model\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_nb = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_nb = accuracy_score(y_test, y_pred_nb) \n",
        "pre_nb = precision_score(y_test, y_pred_nb, average='weighted') \n",
        "rec_nb = recall_score(y_test, y_pred_nb, average='weighted') \n",
        "f1_nb = f1_score(y_test, y_pred_nb, average='weighted') #using weighted average to account for class imbalance\n",
        "report_nb = classification_report(y_test, y_pred_nb)\n",
        "\n",
        "# Print the results\n",
        "print(\"Naive Bayes Results:\")\n",
        "print(f\"Accuracy: {acc_nb}\")\n",
        "print(f\"Precision (weighted): {pre_nb}\")\n",
        "print(f\"Recall (weighted): {rec_nb}\")\n",
        "print(f\"F1-score (weighted): {f1_nb}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report_nb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X5WaLjVttkL"
      },
      "source": [
        "## Model 2 - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vt0jeHhVtv0l"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Results:\n",
            "Accuracy: 0.6074524995388305\n",
            "Precision (weighted): 0.5896436123911377\n",
            "Recall (weighted): 0.5767879175370361\n",
            "F1-score (weighted): 0.6074524995388305\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.52      0.39      0.44       263\n",
            "       adult        0.62      0.32      0.42       118\n",
            "   adventure        0.60      0.24      0.34       155\n",
            "   animation        0.51      0.19      0.28       100\n",
            "   biography        0.00      0.00      0.00        53\n",
            "      comedy        0.57      0.63      0.60      1489\n",
            "       crime        0.32      0.06      0.10       101\n",
            " documentary        0.70      0.85      0.76      2619\n",
            "       drama        0.57      0.76      0.65      2723\n",
            "      family        0.53      0.16      0.25       157\n",
            "     fantasy        0.33      0.08      0.12        65\n",
            "   game-show        0.96      0.59      0.73        39\n",
            "     history        1.00      0.02      0.04        49\n",
            "      horror        0.67      0.64      0.65       441\n",
            "       music        0.64      0.52      0.57       146\n",
            "     musical        0.33      0.02      0.03        55\n",
            "     mystery        0.43      0.05      0.08        64\n",
            "        news        0.33      0.03      0.05        36\n",
            "  reality-tv        0.54      0.31      0.39       177\n",
            "     romance        0.57      0.06      0.11       134\n",
            "      sci-fi        0.53      0.33      0.40       129\n",
            "       short        0.46      0.34      0.39      1015\n",
            "       sport        0.65      0.40      0.49        86\n",
            "   talk-show        0.59      0.26      0.36        78\n",
            "    thriller        0.39      0.17      0.24       318\n",
            "         war        0.67      0.08      0.14        26\n",
            "     western        0.91      0.82      0.86       206\n",
            "\n",
            "     accuracy                           0.61     10842\n",
            "    macro avg       0.55      0.31      0.35     10842\n",
            " weighted avg       0.59      0.61      0.58     10842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)  # Increase max_iter if needed\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr = lr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "pre_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
        "rec_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
        "f1_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
        "report_lr = classification_report(y_test, y_pred_lr)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(f\"Accuracy: {acc_lr}\")\n",
        "print(f\"Precision (weighted): {pre_lr}\")\n",
        "print(f\"Recall (weighted): {rec_lr}\")\n",
        "print(f\"F1-score (weighted): {f1_lr}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiDECHLtyP4"
      },
      "source": [
        "## Model 3 - Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cIooNrJgt0Yw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear SVM Results:\n",
            "Accuracy : 0.5945397528131341\n",
            "Precision (weighted): 0.5653571873988986\n",
            "Recall (weighted): 0.5945397528131341\n",
            "F1-score (weighted): 0.5679010492541673\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.46      0.38      0.42       263\n",
            "       adult        0.56      0.39      0.46       118\n",
            "   adventure        0.47      0.25      0.33       155\n",
            "   animation        0.35      0.20      0.25       100\n",
            "   biography        0.00      0.00      0.00        53\n",
            "      comedy        0.57      0.60      0.58      1489\n",
            "       crime        0.29      0.10      0.15       101\n",
            " documentary        0.70      0.83      0.76      2619\n",
            "       drama        0.58      0.73      0.64      2723\n",
            "      family        0.42      0.17      0.24       157\n",
            "     fantasy        0.29      0.11      0.16        65\n",
            "   game-show        0.83      0.62      0.71        39\n",
            "     history        0.67      0.04      0.08        49\n",
            "      horror        0.62      0.67      0.64       441\n",
            "       music        0.54      0.50      0.52       146\n",
            "     musical        0.21      0.05      0.09        55\n",
            "     mystery        0.20      0.05      0.08        64\n",
            "        news        0.43      0.08      0.14        36\n",
            "  reality-tv        0.47      0.32      0.38       177\n",
            "     romance        0.30      0.06      0.10       134\n",
            "      sci-fi        0.48      0.38      0.42       129\n",
            "       short        0.45      0.32      0.37      1015\n",
            "       sport        0.55      0.45      0.50        86\n",
            "   talk-show        0.50      0.29      0.37        78\n",
            "    thriller        0.34      0.15      0.21       318\n",
            "         war        0.42      0.19      0.26        26\n",
            "     western        0.84      0.86      0.85       206\n",
            "\n",
            "     accuracy                           0.59     10842\n",
            "    macro avg       0.46      0.33      0.36     10842\n",
            " weighted avg       0.57      0.59      0.57     10842\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Initialize and train the Linear SVM model\n",
        "linear_svm_classifier = LinearSVC(random_state=42, max_iter=1000)  # max_iter as needed\n",
        "linear_svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_linear_svm = linear_svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_lsvm = accuracy_score(y_test, y_pred_linear_svm)\n",
        "pre_lsvm = precision_score(y_test, y_pred_linear_svm, average='weighted')\n",
        "rec_lsvm = recall_score(y_test, y_pred_linear_svm, average='weighted')\n",
        "f1_lsvm = f1_score(y_test, y_pred_linear_svm, average='weighted')\n",
        "report_linear_svm = classification_report(y_test, y_pred_linear_svm)\n",
        "\n",
        "# Print the results\n",
        "print(\"Linear SVM Results:\")\n",
        "print(f\"Accuracy : {acc_lsvm}\")\n",
        "print(f\"Precision (weighted): {pre_lsvm}\")\n",
        "print(f\"Recall (weighted): {rec_lsvm}\")\n",
        "print(f\"F1-score (weighted): {f1_lsvm}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report_linear_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.svm import LinearSVC\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# # Define the parameter grid to search\n",
        "# param_grid = {\n",
        "#     'C': [0.01, 0.1, 1],  # Regularization parameter\n",
        "#     'loss': ['hinge', 'squared_hinge'],  # Loss function\n",
        "#     'dual': [True, False]  # Whether to use dual formulation\n",
        "# }\n",
        "\n",
        "# # Initialize the LinearSVC model\n",
        "# linear_svm_classifier = LinearSVC(random_state=42, max_iter=1000)\n",
        "\n",
        "# # Initialize GridSearchCV\n",
        "# grid_search = GridSearchCV(estimator=linear_svm_classifier, param_grid=param_grid, \n",
        "#                            scoring='f1_weighted', cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# # Perform grid search on the training data\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Get the best parameters and model\n",
        "# best_params = grid_search.best_params_\n",
        "# best_model = grid_search.best_estimator_\n",
        "\n",
        "# # Make predictions on the test set using the best model\n",
        "# y_pred_linear_svm = best_model.predict(X_test)\n",
        "\n",
        "# # Evaluate the model\n",
        "# acc_lsvm_best = accuracy_score(y_test, y_pred_linear_svm)\n",
        "# pre_lsvm_best = precision_score(y_test, y_pred_linear_svm, average='weighted')\n",
        "# rec_lsvm_best = recall_score(y_test, y_pred_linear_svm, average='weighted')\n",
        "# f1_lsvm_best = f1_score(y_test, y_pred_linear_svm, average='weighted')\n",
        "# report_linear_svm = classification_report(y_test, y_pred_linear_svm)\n",
        "\n",
        "# # Print the results\n",
        "# print(\"Best Hyperparameters:\", best_params)\n",
        "# print(\"Linear SVM Results with Best Hyperparameters:\")\n",
        "# print(f\"Accuracy : {acc_lsvm}\")\n",
        "# print(f\"Precision (weighted): {pre_lsvm}\")\n",
        "# print(f\"Recall (weighted): {rec_lsvm}\")\n",
        "# print(f\"F1-score (weighted): {f1_lsvm}\")\n",
        "# print(\"Classification Report:\")\n",
        "# print(report_linear_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErH1N9uft4K9"
      },
      "source": [
        "## Model 4 - Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lJ1GXMKut46j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Results:\n",
            "accuracy: 0.5092233905183545\n",
            "Precision (weighted): 0.5474896805652754\n",
            "Recall (weighted): 0.5092233905183545\n",
            "F1-score (weighted): 0.43419036825798385\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.44      0.02      0.03       263\n",
            "       adult        0.80      0.10      0.18       118\n",
            "   adventure        0.71      0.17      0.28       155\n",
            "   animation        0.00      0.00      0.00       100\n",
            "   biography        0.00      0.00      0.00        53\n",
            "      comedy        0.50      0.36      0.42      1489\n",
            "       crime        1.00      0.02      0.04       101\n",
            " documentary        0.58      0.87      0.69      2619\n",
            "       drama        0.43      0.82      0.57      2723\n",
            "      family        0.80      0.03      0.05       157\n",
            "     fantasy        0.00      0.00      0.00        65\n",
            "   game-show        0.91      0.54      0.68        39\n",
            "     history        0.00      0.00      0.00        49\n",
            "      horror        0.66      0.24      0.35       441\n",
            "       music        0.75      0.25      0.37       146\n",
            "     musical        1.00      0.04      0.07        55\n",
            "     mystery        1.00      0.02      0.03        64\n",
            "        news        0.00      0.00      0.00        36\n",
            "  reality-tv        0.75      0.02      0.03       177\n",
            "     romance        0.00      0.00      0.00       134\n",
            "      sci-fi        0.82      0.07      0.13       129\n",
            "       short        0.63      0.12      0.20      1015\n",
            "       sport        0.94      0.19      0.31        86\n",
            "   talk-show        0.57      0.05      0.09        78\n",
            "    thriller        0.67      0.01      0.02       318\n",
            "         war        0.00      0.00      0.00        26\n",
            "     western        0.90      0.54      0.68       206\n",
            "\n",
            "     accuracy                           0.51     10842\n",
            "    macro avg       0.55      0.17      0.19     10842\n",
            " weighted avg       0.55      0.51      0.43     10842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "pre_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "rec_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Random Forest Results:\")\n",
        "print(f\"accuracy: {acc_rf}\")\n",
        "print(f\"Precision (weighted): {pre_rf}\")\n",
        "print(f\"Recall (weighted): {rec_rf}\")\n",
        "print(f\"F1-score (weighted): {f1_rf}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# # Define the parameter grid to search\n",
        "# param_grid = {\n",
        "#     'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "#     'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
        "#     'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "#     'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
        "#     'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
        "#     'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
        "# }\n",
        "\n",
        "# # Initialize the RandomForestClassifier\n",
        "# rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# # Initialize GridSearchCV\n",
        "# grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, \n",
        "#                            scoring='f1_weighted', cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# # Perform grid search on the training data\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Get the best parameters and model\n",
        "# best_params = grid_search.best_params_\n",
        "# best_model = grid_search.best_estimator_\n",
        "\n",
        "# # Make predictions on the test set using the best model\n",
        "# y_pred_rf = best_model.predict(X_test)\n",
        "\n",
        "# # Evaluate the model\n",
        "# f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "# report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "# # Print the results\n",
        "# print(\"Best Hyperparameters:\", best_params)\n",
        "# print(\"Random Forest Results with Best Hyperparameters:\")\n",
        "# print(f\"F1-score (weighted): {f1_rf}\")\n",
        "# print(\"Classification Report:\")\n",
        "# print(report_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIYcZwEht88u"
      },
      "source": [
        "## Model 5 - XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N4t2RKost_hJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Results:\n",
            "F1-score (weighted): 0.5667773473528869\n",
            "F1-score (weighted): 0.5488471706977178\n",
            "F1-score (weighted): 0.5667773473528869\n",
            "F1-score (weighted): 0.5344488497740835\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      action        0.45      0.28      0.35       263\n",
            "       adult        0.51      0.30      0.38       118\n",
            "   adventure        0.54      0.26      0.35       155\n",
            "   animation        0.40      0.14      0.21       100\n",
            "   biography        1.00      0.02      0.04        53\n",
            "      comedy        0.54      0.50      0.52      1489\n",
            "       crime        0.20      0.05      0.08       101\n",
            " documentary        0.66      0.81      0.73      2619\n",
            "       drama        0.51      0.77      0.61      2723\n",
            "      family        0.47      0.13      0.20       157\n",
            "     fantasy        0.25      0.06      0.10        65\n",
            "   game-show        0.92      0.56      0.70        39\n",
            "     history        0.00      0.00      0.00        49\n",
            "      horror        0.65      0.56      0.60       441\n",
            "       music        0.60      0.49      0.54       146\n",
            "     musical        0.44      0.07      0.12        55\n",
            "     mystery        0.21      0.05      0.08        64\n",
            "        news        0.27      0.11      0.16        36\n",
            "  reality-tv        0.48      0.24      0.32       177\n",
            "     romance        0.21      0.04      0.06       134\n",
            "      sci-fi        0.50      0.30      0.38       129\n",
            "       short        0.52      0.27      0.36      1015\n",
            "       sport        0.63      0.48      0.54        86\n",
            "   talk-show        0.42      0.26      0.32        78\n",
            "    thriller        0.34      0.16      0.21       318\n",
            "         war        0.60      0.12      0.19        26\n",
            "     western        0.88      0.78      0.83       206\n",
            "\n",
            "     accuracy                           0.57     10842\n",
            "    macro avg       0.49      0.29      0.33     10842\n",
            " weighted avg       0.55      0.57      0.53     10842\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
        "xgb_classifier.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb_encoded = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Decode the predictions back to original labels\n",
        "y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb_encoded)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "precision_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\n",
        "recall_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\n",
        "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
        "report_xgb = classification_report(y_test, y_pred_xgb)\n",
        "\n",
        "# Print the results\n",
        "print(\"XGBoost Results:\")\n",
        "print(f\"F1-score (weighted): {accuracy_xgb}\")\n",
        "print(f\"F1-score (weighted): {precision_xgb}\")\n",
        "print(f\"F1-score (weighted): {recall_xgb}\")\n",
        "print(f\"F1-score (weighted): {f1_xgb}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report_xgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 6 - Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    tfidf_features, df['genre'], test_size=0.1, random_state=42, stratify=df['genre']\n",
        ")\n",
        "\n",
        "# Label encode and one-hot encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_onehot = onehot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))\n",
        "y_test_onehot = onehot_encoder.transform(y_test_encoded.reshape(-1, 1))\n",
        "\n",
        "# Create the neural network model with specified parameters\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(64, activation='relu'), # half of 128\n",
        "    keras.layers.Dropout(0.15), # half of 0.3\n",
        "    keras.layers.Dense(y_train_onehot.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train.toarray(), y_train_onehot, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred_test_onehot = model.predict(X_test.toarray())\n",
        "y_pred_test = np.argmax(y_pred_test_onehot, axis=1)\n",
        "y_test_decoded = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "acc_test = accuracy_score(y_test_decoded, y_pred_test)\n",
        "pre_test = precision_score(y_test_decoded, y_pred_test, average='weighted')\n",
        "rec_test = recall_score(y_test_decoded, y_pred_test, average='weighted')\n",
        "f1_test = f1_score(y_test_decoded, y_pred_test, average='weighted')\n",
        "report_test = classification_report(y_test_decoded, y_pred_test)\n",
        "\n",
        "print(\"Neural Network Test Results:\")\n",
        "print(f\"Accuracy: {acc_test}\")\n",
        "print(f\"Precision (weighted): {pre_test}\")\n",
        "print(f\"Recall(weighted): {rec_test}\")\n",
        "print(f\"F1-score (weighted): {f1_test}\")\n",
        "print(\"Classification Report:\\n\", report_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scikit learn mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tfidf_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, classification_report\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[1;32m      8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtfidf_features\u001b[49m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert TF-IDF sparse matrices to NumPy arrays\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_train_array \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mtoarray()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf_features' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    tfidf_features, df['genre'], test_size=0.1, random_state=42, stratify=df['genre']\n",
        ")\n",
        "\n",
        "# Convert TF-IDF sparse matrices to NumPy arrays\n",
        "X_train_array = X_train.toarray()\n",
        "X_test_array = X_test.toarray()\n",
        "\n",
        "# Label encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Create the MLP model with specified parameters\n",
        "model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam',\n",
        "                      alpha=0.0001, batch_size=32, learning_rate='adaptive', max_iter=5,\n",
        "                      random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_array, y_train_encoded)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred_test = model.predict(X_test_array)\n",
        "\n",
        "acc_test = accuracy_score(y_test_encoded, y_pred_test)\n",
        "pre_test = precision_score(y_test_encoded, y_pred_test, average='weighted')\n",
        "rec_test = recall_score(y_test_encoded, y_pred_test, average='weighted')\n",
        "f1_test = f1_score(y_test_encoded, y_pred_test, average='weighted')\n",
        "report_test = classification_report(y_test_encoded, y_pred_test)\n",
        "\n",
        "print(\"scikit-learn MLP Test Results:\")\n",
        "print(f\"Accuracy: {acc_test}\")\n",
        "print(f\"Precision (weighted): {pre_test}\")\n",
        "print(f\"Recall(weighted): {rec_test}\")\n",
        "print(f\"F1-score (weighted): {f1_test}\")\n",
        "print(\"Classification Report:\\n\", report_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Embeddings (GLoVe6B - 100d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd  # Assuming you're using pandas DataFrames\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# 1. Load GloVe Embeddings\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings('glove.6B.100d.txt')  # Replace with your path\n",
        "embeddings_dim = 100  # Adjust to your GloVe dimension\n",
        "\n",
        "# 2. Convert Text to Embedding Vectors\n",
        "def text_to_embedding_vectors(texts, embeddings, embeddings_dim):\n",
        "    embedding_vectors = []\n",
        "    for text in texts:\n",
        "        words = text.split()  # Assuming already lowercased and processed\n",
        "        vectors = [embeddings.get(word, np.zeros(embeddings_dim)) for word in words]\n",
        "        if vectors:\n",
        "            text_vector = np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            text_vector = np.zeros(embeddings_dim)\n",
        "        embedding_vectors.append(text_vector)\n",
        "    return np.array(embedding_vectors)\n",
        "\n",
        "X_embedded = text_to_embedding_vectors(df['processed_description'], glove_embeddings, embeddings_dim)\n",
        "\n",
        "# 3. Prepare Data and Train/Test Split\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df['genre'])\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "y_onehot = onehot_encoder.fit_transform(y_encoded.reshape(-1, 1))\n",
        "\n",
        "X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot = train_test_split(\n",
        "    X_embedded, y_onehot, test_size=0.1, random_state=42, stratify=df['genre']\n",
        ")\n",
        "\n",
        "# X_train_embedded and X_test_embedded now contain the GloVe embedding vectors,\n",
        "# y_train_onehot and y_test_onehot contain the one-hot encoded genre labels.\n",
        "# You can use these variables to train and evaluate any machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1 - Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes with GloVe Embeddings Results:\n",
            "Accuracy: 0.3827707065117137\n",
            "Precision (weighted): 0.1916778371891662\n",
            "Recall (weighted): 0.3827707065117137\n",
            "F1-score (weighted): 0.25423458140320393\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       263\n",
            "           1       0.00      0.00      0.00       118\n",
            "           2       0.00      0.00      0.00       155\n",
            "           3       0.00      0.00      0.00       100\n",
            "           4       0.00      0.00      0.00        53\n",
            "           5       0.00      0.00      0.00      1489\n",
            "           6       0.00      0.00      0.00       101\n",
            "           7       0.43      0.75      0.55      2619\n",
            "           8       0.35      0.80      0.48      2723\n",
            "           9       0.00      0.00      0.00       157\n",
            "          10       0.00      0.00      0.00        65\n",
            "          11       0.00      0.00      0.00        39\n",
            "          12       0.00      0.00      0.00        49\n",
            "          13       0.00      0.00      0.00       441\n",
            "          14       0.00      0.00      0.00       146\n",
            "          15       0.00      0.00      0.00        55\n",
            "          16       0.00      0.00      0.00        64\n",
            "          17       0.00      0.00      0.00        36\n",
            "          18       0.00      0.00      0.00       177\n",
            "          19       0.00      0.00      0.00       134\n",
            "          20       0.00      0.00      0.00       129\n",
            "          21       0.00      0.00      0.00      1015\n",
            "          22       0.00      0.00      0.00        86\n",
            "          23       0.00      0.00      0.00        78\n",
            "          24       0.00      0.00      0.00       318\n",
            "          25       0.00      0.00      0.00        26\n",
            "          26       0.00      0.00      0.00       206\n",
            "\n",
            "    accuracy                           0.38     10842\n",
            "   macro avg       0.03      0.06      0.04     10842\n",
            "weighted avg       0.19      0.38      0.25     10842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot from the previous GloVe embedding code\n",
        "\n",
        "# Convert one-hot encoded labels back to label encoded format for Naive Bayes\n",
        "y_train_encoded = np.argmax(y_train_onehot, axis=1)\n",
        "y_test_encoded = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# Initialize and train the Naive Bayes model\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Naive Bayes requires non-negative input, so we'll use absolute values\n",
        "# GloVe embeddings can have negative values.\n",
        "X_train_embedded_abs = np.abs(X_train_embedded)\n",
        "X_test_embedded_abs = np.abs(X_test_embedded)\n",
        "\n",
        "nb_classifier.fit(X_train_embedded_abs, y_train_encoded)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_nb = nb_classifier.predict(X_test_embedded_abs)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_nb = accuracy_score(y_test_encoded, y_pred_nb)\n",
        "pre_nb = precision_score(y_test_encoded, y_pred_nb, average='weighted')\n",
        "rec_nb = recall_score(y_test_encoded, y_pred_nb, average='weighted')\n",
        "f1_nb = f1_score(y_test_encoded, y_pred_nb, average='weighted')\n",
        "report_nb = classification_report(y_test_encoded, y_pred_nb)\n",
        "\n",
        "# Print the results\n",
        "print(\"Naive Bayes with GloVe Embeddings Results:\")\n",
        "print(f\"Accuracy: {acc_nb}\")\n",
        "print(f\"Precision (weighted): {pre_nb}\")\n",
        "print(f\"Recall (weighted): {rec_nb}\")\n",
        "print(f\"F1-score (weighted): {f1_nb}\")\n",
        "print(\"Classification Report:\\n\", report_nb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression with GloVe Embeddings Results:\n",
            "Accuracy: 0.4961261759822911\n",
            "Precision (weighted): 0.4587705069747756\n",
            "Recall (weighted): 0.4961261759822911\n",
            "F1-score (weighted): 0.45639040932674213\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.29      0.35       263\n",
            "           1       0.31      0.11      0.16       118\n",
            "           2       0.15      0.04      0.06       155\n",
            "           3       0.42      0.15      0.22       100\n",
            "           4       0.00      0.00      0.00        53\n",
            "           5       0.40      0.42      0.41      1489\n",
            "           6       0.19      0.03      0.05       101\n",
            "           7       0.58      0.77      0.66      2619\n",
            "           8       0.49      0.69      0.57      2723\n",
            "           9       0.38      0.06      0.11       157\n",
            "          10       0.14      0.03      0.05        65\n",
            "          11       0.72      0.59      0.65        39\n",
            "          12       0.33      0.02      0.04        49\n",
            "          13       0.48      0.38      0.42       441\n",
            "          14       0.51      0.42      0.46       146\n",
            "          15       0.20      0.02      0.03        55\n",
            "          16       0.00      0.00      0.00        64\n",
            "          17       0.25      0.03      0.05        36\n",
            "          18       0.40      0.14      0.21       177\n",
            "          19       0.26      0.04      0.07       134\n",
            "          20       0.43      0.23      0.30       129\n",
            "          21       0.39      0.20      0.27      1015\n",
            "          22       0.47      0.40      0.43        86\n",
            "          23       0.46      0.23      0.31        78\n",
            "          24       0.31      0.09      0.14       318\n",
            "          25       0.33      0.08      0.12        26\n",
            "          26       0.69      0.70      0.69       206\n",
            "\n",
            "    accuracy                           0.50     10842\n",
            "   macro avg       0.36      0.23      0.25     10842\n",
            "weighted avg       0.46      0.50      0.46     10842\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot from the GloVe embedding code\n",
        "\n",
        "# Convert one-hot encoded labels back to label encoded format for Logistic Regression\n",
        "y_train_encoded = np.argmax(y_train_onehot, axis=1)\n",
        "y_test_encoded = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "lr_classifier = LogisticRegression(random_state=42, max_iter=1000) # Increased max_iter\n",
        "\n",
        "lr_classifier.fit(X_train_embedded, y_train_encoded)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lr = lr_classifier.predict(X_test_embedded)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_lr = accuracy_score(y_test_encoded, y_pred_lr)\n",
        "pre_lr = precision_score(y_test_encoded, y_pred_lr, average='weighted')\n",
        "rec_lr = recall_score(y_test_encoded, y_pred_lr, average='weighted')\n",
        "f1_lr = f1_score(y_test_encoded, y_pred_lr, average='weighted')\n",
        "report_lr = classification_report(y_test_encoded, y_pred_lr)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression with GloVe Embeddings Results:\")\n",
        "print(f\"Accuracy: {acc_lr}\")\n",
        "print(f\"Precision (weighted): {pre_lr}\")\n",
        "print(f\"Recall (weighted): {rec_lr}\")\n",
        "print(f\"F1-score (weighted): {f1_lr}\")\n",
        "print(\"Classification Report:\\n\", report_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 3 - Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear SVM with GloVe Embeddings Results:\n",
            "Accuracy: 0.4803\n",
            "Precision (weighted): 0.4164\n",
            "Recall (weighted): 0.4803\n",
            "F1-score (weighted): 0.4141\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.16      0.25       263\n",
            "           1       0.57      0.03      0.06       118\n",
            "           2       0.00      0.00      0.00       155\n",
            "           3       0.00      0.00      0.00       100\n",
            "           4       0.00      0.00      0.00        53\n",
            "           5       0.39      0.36      0.37      1489\n",
            "           6       0.00      0.00      0.00       101\n",
            "           7       0.53      0.82      0.65      2619\n",
            "           8       0.46      0.71      0.56      2723\n",
            "           9       0.00      0.00      0.00       157\n",
            "          10       0.00      0.00      0.00        65\n",
            "          11       0.74      0.59      0.66        39\n",
            "          12       0.00      0.00      0.00        49\n",
            "          13       0.43      0.28      0.34       441\n",
            "          14       0.42      0.28      0.34       146\n",
            "          15       0.00      0.00      0.00        55\n",
            "          16       0.00      0.00      0.00        64\n",
            "          17       0.00      0.00      0.00        36\n",
            "          18       0.00      0.00      0.00       177\n",
            "          19       0.00      0.00      0.00       134\n",
            "          20       0.54      0.15      0.23       129\n",
            "          21       0.41      0.14      0.20      1015\n",
            "          22       0.44      0.31      0.37        86\n",
            "          23       0.60      0.04      0.07        78\n",
            "          24       0.33      0.01      0.01       318\n",
            "          25       0.00      0.00      0.00        26\n",
            "          26       0.65      0.71      0.68       206\n",
            "\n",
            "    accuracy                           0.48     10842\n",
            "   macro avg       0.26      0.17      0.18     10842\n",
            "weighted avg       0.42      0.48      0.41     10842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot from the GloVe embedding code\n",
        "\n",
        "# Convert one-hot encoded labels back to label encoded format for Linear SVM\n",
        "y_train_encoded = np.argmax(y_train_onehot, axis=1)\n",
        "y_test_encoded = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# Initialize and train the Linear SVM model\n",
        "svm_classifier = LinearSVC(random_state=42, max_iter=1000)  # Increased max_iter\n",
        "svm_classifier.fit(X_train_embedded, y_train_encoded)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_classifier.predict(X_test_embedded)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_svm = accuracy_score(y_test_encoded, y_pred_svm)\n",
        "pre_svm = precision_score(y_test_encoded, y_pred_svm, average='weighted')\n",
        "rec_svm = recall_score(y_test_encoded, y_pred_svm, average='weighted')\n",
        "f1_svm = f1_score(y_test_encoded, y_pred_svm, average='weighted')\n",
        "report_svm = classification_report(y_test_encoded, y_pred_svm)\n",
        "\n",
        "# Print the results\n",
        "print(\"Linear SVM with GloVe Embeddings Results:\")\n",
        "print(f\"Accuracy: {acc_svm:.4f}\")\n",
        "print(f\"Precision (weighted): {pre_svm:.4f}\")\n",
        "print(f\"Recall (weighted): {rec_svm:.4f}\")\n",
        "print(f\"F1-score (weighted): {f1_svm:.4f}\")\n",
        "print(\"Classification Report:\\n\", report_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 4 - Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest with GloVe Embeddings Results:\n",
            "Accuracy: 0.4517616675890057\n",
            "Precision (weighted): 0.4746915934366851\n",
            "Recall (weighted): 0.4517616675890057\n",
            "F1-score (weighted): 0.3754167299667026\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.02      0.04       263\n",
            "           1       0.00      0.00      0.00       118\n",
            "           2       1.00      0.02      0.04       155\n",
            "           3       0.00      0.00      0.00       100\n",
            "           4       0.00      0.00      0.00        53\n",
            "           5       0.36      0.32      0.34      1489\n",
            "           6       0.00      0.00      0.00       101\n",
            "           7       0.50      0.83      0.62      2619\n",
            "           8       0.42      0.70      0.53      2723\n",
            "           9       0.67      0.01      0.03       157\n",
            "          10       0.00      0.00      0.00        65\n",
            "          11       0.94      0.44      0.60        39\n",
            "          12       0.00      0.00      0.00        49\n",
            "          13       0.49      0.09      0.15       441\n",
            "          14       0.64      0.29      0.40       146\n",
            "          15       1.00      0.04      0.07        55\n",
            "          16       1.00      0.02      0.03        64\n",
            "          17       0.00      0.00      0.00        36\n",
            "          18       1.00      0.02      0.03       177\n",
            "          19       1.00      0.01      0.03       134\n",
            "          20       0.57      0.03      0.06       129\n",
            "          21       0.46      0.10      0.17      1015\n",
            "          22       0.71      0.23      0.35        86\n",
            "          23       1.00      0.04      0.07        78\n",
            "          24       0.60      0.01      0.02       318\n",
            "          25       0.00      0.00      0.00        26\n",
            "          26       0.82      0.34      0.48       206\n",
            "\n",
            "    accuracy                           0.45     10842\n",
            "   macro avg       0.50      0.13      0.15     10842\n",
            "weighted avg       0.47      0.45      0.38     10842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot from the GloVe embedding code\n",
        "\n",
        "# Convert one-hot encoded labels back to label encoded format for Random Forest\n",
        "y_train_encoded = np.argmax(y_train_onehot, axis=1)\n",
        "y_test_encoded = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_classifier.fit(X_train_embedded, y_train_encoded)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test_embedded)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_rf = accuracy_score(y_test_encoded, y_pred_rf)\n",
        "pre_rf = precision_score(y_test_encoded, y_pred_rf, average='weighted')\n",
        "rec_rf = recall_score(y_test_encoded, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test_encoded, y_pred_rf, average='weighted')\n",
        "report_rf = classification_report(y_test_encoded, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(\"Random Forest with GloVe Embeddings Results:\")\n",
        "print(f\"Accuracy: {acc_rf}\")\n",
        "print(f\"Precision (weighted): {pre_rf}\")\n",
        "print(f\"Recall (weighted): {rec_rf}\")\n",
        "print(f\"F1-score (weighted): {f1_rf}\")\n",
        "print(\"Classification Report:\\n\", report_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 5 - XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost with GloVe Embeddings Results:\n",
            "F1-score (weighted): 0.48930086699870873\n",
            "F1-score (weighted): 0.46097652509648374\n",
            "F1-score (weighted): 0.48930086699870873\n",
            "F1-score (weighted): 0.4526596865964386\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.29      0.33       263\n",
            "           1       0.28      0.08      0.13       118\n",
            "           2       0.37      0.12      0.18       155\n",
            "           3       0.54      0.13      0.21       100\n",
            "           4       0.00      0.00      0.00        53\n",
            "           5       0.40      0.44      0.41      1489\n",
            "           6       0.21      0.03      0.05       101\n",
            "           7       0.57      0.77      0.66      2619\n",
            "           8       0.48      0.65      0.55      2723\n",
            "           9       0.33      0.06      0.11       157\n",
            "          10       0.11      0.02      0.03        65\n",
            "          11       0.84      0.54      0.66        39\n",
            "          12       0.00      0.00      0.00        49\n",
            "          13       0.48      0.34      0.40       441\n",
            "          14       0.65      0.44      0.52       146\n",
            "          15       0.33      0.04      0.07        55\n",
            "          16       1.00      0.02      0.03        64\n",
            "          17       0.00      0.00      0.00        36\n",
            "          18       0.37      0.12      0.18       177\n",
            "          19       0.33      0.05      0.09       134\n",
            "          20       0.45      0.22      0.30       129\n",
            "          21       0.35      0.22      0.27      1015\n",
            "          22       0.61      0.38      0.47        86\n",
            "          23       0.50      0.21      0.29        78\n",
            "          24       0.26      0.08      0.13       318\n",
            "          25       0.00      0.00      0.00        26\n",
            "          26       0.78      0.66      0.71       206\n",
            "\n",
            "    accuracy                           0.49     10842\n",
            "   macro avg       0.39      0.22      0.25     10842\n",
            "weighted avg       0.46      0.49      0.45     10842\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot from the GloVe embedding code\n",
        "\n",
        "# Convert one-hot encoded labels back to label encoded format for XGBoost\n",
        "y_train_encoded = np.argmax(y_train_onehot, axis=1)\n",
        "y_test_encoded = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "xgb_classifier.fit(X_train_embedded, y_train_encoded)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb_classifier.predict(X_test_embedded)\n",
        "\n",
        "# Evaluate the model\n",
        "acc_xgb = accuracy_score(y_test_encoded, y_pred_xgb)\n",
        "pre_xgb = precision_score(y_test_encoded, y_pred_xgb, average='weighted')\n",
        "rec_xgb = recall_score(y_test_encoded, y_pred_xgb, average='weighted')\n",
        "f1_xgb = f1_score(y_test_encoded, y_pred_xgb, average='weighted')\n",
        "report_xgb = classification_report(y_test_encoded, y_pred_xgb)\n",
        "\n",
        "# Print the results\n",
        "print(\"XGBoost with GloVe Embeddings Results:\")\n",
        "print(f\"F1-score (weighted): {acc_xgb}\")\n",
        "print(f\"F1-score (weighted): {pre_xgb}\")\n",
        "print(f\"F1-score (weighted): {rec_xgb}\")\n",
        "print(f\"F1-score (weighted): {f1_xgb}\")\n",
        "print(\"Classification Report:\\n\", report_xgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 6 - Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot from the GloVe embedding code\n",
        "\n",
        "# Create the neural network model with what I think are good parameters\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation='relu', input_shape=(X_train_embedded.shape[1],)),  # Increased hidden units\n",
        "    keras.layers.Dropout(0.4),  # Increased dropout\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.3), # Increased dropout\n",
        "    keras.layers.Dense(y_train_onehot.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_embedded, y_train_onehot, epochs=10, batch_size=32, verbose=1) # Increased epochs\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred_test_onehot = model.predict(X_test_embedded)\n",
        "y_pred_test = np.argmax(y_pred_test_onehot, axis=1)\n",
        "y_test_decoded = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "f1_test = f1_score(y_test_decoded, y_pred_test, average='weighted')\n",
        "report_test = classification_report(y_test_decoded, y_pred_test)\n",
        "\n",
        "print(\"Neural Network with GloVe Embeddings Results:\")\n",
        "print(f\"F1-score (weighted): {f1_test}\")\n",
        "print(\"Classification Report:\\n\", report_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scikit learn mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scikit-learn MLP with GloVe Embeddings Results:\n",
            "Accuracy: 0.421877882309537\n",
            "Precision (weighted): 0.41426699998270117\n",
            "Recall (weighted): 0.421877882309537\n",
            "F1-score (weighted): 0.4174792924874099\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.34      0.34       263\n",
            "           1       0.21      0.18      0.19       118\n",
            "           2       0.21      0.24      0.22       155\n",
            "           3       0.21      0.22      0.21       100\n",
            "           4       0.00      0.00      0.00        53\n",
            "           5       0.35      0.37      0.36      1489\n",
            "           6       0.16      0.10      0.12       101\n",
            "           7       0.59      0.63      0.61      2619\n",
            "           8       0.48      0.48      0.48      2723\n",
            "           9       0.15      0.13      0.14       157\n",
            "          10       0.05      0.06      0.05        65\n",
            "          11       0.58      0.56      0.57        39\n",
            "          12       0.03      0.02      0.02        49\n",
            "          13       0.36      0.41      0.38       441\n",
            "          14       0.47      0.47      0.47       146\n",
            "          15       0.11      0.09      0.10        55\n",
            "          16       0.04      0.02      0.02        64\n",
            "          17       0.03      0.03      0.03        36\n",
            "          18       0.24      0.22      0.23       177\n",
            "          19       0.15      0.16      0.16       134\n",
            "          20       0.28      0.29      0.28       129\n",
            "          21       0.28      0.23      0.25      1015\n",
            "          22       0.46      0.47      0.46        86\n",
            "          23       0.28      0.29      0.29        78\n",
            "          24       0.15      0.13      0.14       318\n",
            "          25       0.28      0.19      0.23        26\n",
            "          26       0.70      0.66      0.68       206\n",
            "\n",
            "    accuracy                           0.42     10842\n",
            "   macro avg       0.26      0.26      0.26     10842\n",
            "weighted avg       0.41      0.42      0.42     10842\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have X_train_embedded, X_test_embedded, y_train_onehot, y_test_onehot from the GloVe embedding code\n",
        "\n",
        "# Convert one-hot encoded labels to integer labels\n",
        "y_train = np.argmax(y_train_onehot, axis=1)\n",
        "y_test = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# Create the MLP model with improved parameters\n",
        "model = MLPClassifier(hidden_layer_sizes=(512, 256, 128), activation='relu', solver='adam',\n",
        "                      alpha=0.0001, batch_size=64, learning_rate='adaptive', max_iter=200,\n",
        "                      random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_embedded, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred_test = model.predict(X_test_embedded)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
        "recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
        "report_test = classification_report(y_test, y_pred_test)\n",
        "\n",
        "print(\"scikit-learn MLP with GloVe Embeddings Results:\")\n",
        "print(f\"Accuracy: {accuracy_test}\")\n",
        "print(f\"Precision (weighted): {precision_test}\")\n",
        "print(f\"Recall (weighted): {recall_test}\")\n",
        "print(f\"F1-score (weighted): {f1_test}\")\n",
        "print(\"Classification Report:\\n\", report_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer (DistilBERT)`\n",
        "- ran in google colab - weighted F-score: 0.61"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "\n",
        "# 1. Data Preparation and Preprocessing\n",
        "label_encoder = LabelEncoder()\n",
        "df['genre_encoded'] = label_encoder.fit_transform(df['genre'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['processed_description'], df['genre_encoded'], test_size=0.1, random_state=42, stratify=df['genre']\n",
        ")\n",
        "\n",
        "# 2. Tokenization\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize_data(texts, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'][0])\n",
        "        attention_masks.append(encoded['attention_mask'][0])\n",
        "    return tf.stack(input_ids), tf.stack(attention_masks)\n",
        "\n",
        "X_train_ids, X_train_masks = tokenize_data(X_train)\n",
        "X_test_ids, X_test_masks = tokenize_data(X_test)\n",
        "\n",
        "# 3. Transformer Model\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['genre'].unique()))\n",
        "\n",
        "# 4. Training\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    [X_train_ids, X_train_masks],\n",
        "    y_train,\n",
        "    epochs=3,\n",
        "    batch_size=16,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# 5. Evaluation\n",
        "logits = model.predict([X_test_ids, X_test_masks]).logits\n",
        "y_pred = np.argmax(logits, axis=1)\n",
        "\n",
        "acc = f1_score(y_test, y_pred)\n",
        "pre = f1_score(y_test, y_pred, average='weighted')\n",
        "rec = f1_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"F1-score (weighted): {acc}\")\n",
        "print(f\"F1-score (weighted): {pre}\")\n",
        "print(f\"F1-score (weighted): {rec}\")\n",
        "print(f\"F1-score (weighted): {f1}\")\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
